#!/bin/bash
# 补齐论文缺失内容

cd /Users/fenix/github/nex/ai_saga

# 论文1: The Perceptron (1958)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = '在感知机之前，AI领域处于理论探索阶段。1956年达特茅斯会议刚刚提出人工智能概念，计算机主要用于符号计算和逻辑推理。没有能够从数据中学习模式的实用机器学习算法。',
  core_contribution = '提出了第一个能够从示例中学习的实用神经网络算法——感知机学习规则。证明了简单的神经元模型可以通过迭代调整权重来学习分类模式，开创了连接主义范式的先河。',
  core_mechanism = '感知机接收多个输入（模拟神经元树突），每个输入乘以权重后求和。如果总和超过阈值则输出1，否则输出0。学习规则根据预测误差调整权重：当预测错误时，权重向正确方向移动。',
  why_it_wins = '这是第一个从数据中自动学习的算法，不需要显式编程。相比手工编写的规则，它能从训练样本中自动发现模式。虽然后来被证明有局限性，但它点燃了神经网络研究的火种，为后续的反向传播、深度学习奠定了基础。'
WHERE id = 1;
EOF

# 论文2: Perceptrons (1969)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = '1958-1969年间，感知机被认为是非常强大的学习机器。研究人员对其能力过于乐观，认为它很快就能解决复杂问题。很多研究资金投入到基于感知机的神经网络研究中。',
  core_contribution = '从数学上严格证明了单层感知机的根本局限性：无法解决线性不可分问题，特别是无法学习XOR函数。指出多层网络可能有更强的能力，但没有提供训练方法。',
  core_mechanism = '使用数学证明展示了感知机的决策边界是线性的，而XOR问题需要非线性边界。证明了对于某些模式，单层感知机无论如何调整权重都无法正确分类。',
  why_it_wins = '虽然这是负面结果，但它是严谨的数学证明而非猜测。它终结了感知机过度炒作，迫使研究人员寻找新的方向。更重要的是，它暗示了解决方案——多层网络，只是当时还没有找到训练它们的方法。这本书直接导致了第一次AI寒冬，但也指明了后续反向传播研究的方向。'
WHERE id = 2;
EOF

# 论文3: Backpropagation (1986)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = '1969年Minsky和Papert指出感知机局限性后，神经网络研究进入寒冬。虽然有研究人员尝试多层网络，但缺乏有效的训练方法。符号AI和专家系统在此期间占据主导。',
  core_contribution = '推广了反向传播算法，解决了训练多层神经网络的核心问题。通过链式法则高效计算误差相对于每个权重的梯度，使得任意深度网络的训练成为可能。',
  core_mechanism = '前向传播计算网络输出和误差，然后通过链式法则将误差信号从输出层反向传播到输入层，计算每个权重的梯度。权重沿着梯度下降方向更新，最小化整体误差。',
  why_it_wins = '这是感知机局限性的解决方案！多层网络可以学习非线性函数，解决了XOR等问题。虽然早期被忽视，但1986年的推广恰逢计算能力提升，直接引发了连接主义复兴和第二波神经网络热潮，为后来的深度学习奠定了基础。'
WHERE id = 3;
EOF

# 论文4: Gradient Vanishing (1994)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = '反向传播复兴神经网络后，研究人员尝试用它训练循环神经网络(RNN)处理序列数据。初期在短序列上有些成功，但长序列学习一直是个难题。LSTM等现代技术尚未出现。',
  core_contribution = '从理论上分析了为什么RNN难以学习长期依赖。证明在反向传播通过时间的过程中，梯度会指数级衰减（消失）或增长（爆炸），导致模型无法捕捉远距离的依赖关系。',
  core_mechanism = '通过数学分析展示了误差信号在时间步之间传播时的行为。梯度是多个雅可比矩阵的乘积，其范数会指数级变化，导致网络要么遗忘早期信息，要么被早期信息主导。',
  why_it_wins = '这是第一次从理论上解释了RNN的根本问题，不是工程技巧能解决的。这个负面结果直接推动了LSTM、门控机制、注意力机制的研究，为序列建模指明了新的方向。没有这篇论文，就不会有后来的Transformer。'
WHERE id = 4;
EOF

# 论文5: LSTM (1997)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = '1994年梯度消失论文揭示了RNN的根本问题：无法学习长期依赖。虽然知道问题所在，但缺乏有效的解决方案。简单RNN在语音识别、机器翻译等长序列任务上表现不佳。',
  core_contribution = '提出了LSTM架构，通过引入门控机制和细胞状态解决了梯度消失问题。这是第一个能够稳定学习跨越数千时间步依赖关系的RNN变体。',
  core_mechanism = '细胞状态作为信息传送带，通过遗忘门、输入门、输出门三个门控控制信息流。门使用sigmoid激活(0-1)决定保留或丢弃信息。梯度可以几乎不变地沿细胞状态反向传播，避免了消失问题。',
  why_it_wins = 'LSTM直接解决了梯度消失问题，让RNN真正实用化。它在语音识别、机器翻译、手写识别等任务上取得了突破性成果，主导了序列建模领域近20年，直到被Transformer取代。今天仍在广泛使用。'
WHERE id = 5;
EOF

# 论文6: AlexNet (2012)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = '2000年代深度学习虽然理论上可行，但受限于计算能力和数据规模。计算机视觉领域被SVM主导，特征工程(SIFT、HOG)是标准做法。神经网络被认为不实用。ImageNet竞赛是当时最大的视觉识别挑战。',
  core_contribution = '首次在大规模图像识别上证明了深度CNN的威力。在ImageNet竞赛中以巨大优势获胜，将错误率从26%降至15%，震惊了整个计算机视觉界。',
  core_mechanism = '8层深度卷积神经网络，使用ReLU激活解决梯度消失问题，采用dropout正则化防止过拟合，利用GPU加速训练(比CPU快40倍)。数据增强和精心设计的网络架构至关重要。',
  why_it_wins = '这是深度学习的 tipping point！证明了深度网络+GPU+大数据可以改变游戏规则。一夜之间计算机视觉界从手工特征转向深度学习，研究资金和人才大量涌入，开启了现代深度学习时代。如果没有AlexNet，AI革命可能要晚很多年。'
WHERE id = 6;
EOF

# 论文7: ResNet (2015)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = 'AlexNet成功后，研究人员开始堆叠更多层以提高性能。但发现超过20层后，更深的网络反而训练误差更高——这是退化问题，不是过拟合。梯度消失/爆炸让超深网络难以训练。',
  core_contribution = '提出了残差学习框架，通过跳跃连接让网络学习残差映射而非直接映射。这是第一个能够稳定训练100+层甚至1000+层网络的方法。',
  core_mechanism = '残差块包含跳跃连接(快捷连接)，将输入直接加到卷积层输出上。网络学习F(x)=H(x)-x而非直接学习H(x)。恒等映射易于学习，深层网络的优化变得更容易，梯度可以直接流过跳跃连接。',
  why_it_wins = 'ResNet解决了深度学习的核心瓶颈——网络深度。超深网络带来更强的表达能力，ResNet成为现代计算机视觉的主干架构，影响了所有后续深度学习领域。没有ResNet，可能就没有今天的大型模型。'
WHERE id = 7;
EOF

# 论文8: Transformer (2017)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = '2017年前，序列建模完全依赖RNN、LSTM和CNN。虽然注意力机制已被证明有用，但只是作为RNN的增强组件。LSTM虽然解决了梯度消失，但顺序计算限制了并行性和长距离依赖的捕捉。',
  core_contribution = '提出了完全基于注意力机制的Transformer架构，彻底消除了循环和卷积。证明了注意力本身就是足够的，开创了序列建模的新范式。',
  core_mechanism = '自注意力机制让每个位置关注所有其他位置，多头注意力允许关注不同的表示子空间。位置编码注入序列顺序信息。整个架构高度并行化，在GPU上训练效率远超RNN。',
  why_it_wins = 'Transformer改变了整个NLP领域，BERT、GPT和所有现代大语言模型都基于此。它也被应用于视觉(ViT)、音频和多模态，成为现代AI的基础架构。可以说，这是深度学习历史上最重要的架构创新。'
WHERE id = 8;
EOF

# 论文9: BERT (2018)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = 'Transformer出现后，预训练+微调成为标准范式，但GPT使用单向(从左到右)注意力。双向上下文能否给出更好的表示是开放问题。ELMo虽然双向，但架构不同。',
  core_contribution = '引入了双向编码器预训练，通过掩码语言模型(MLM)同时利用左右上下文。在11个NLP任务上取得SOTA，开创了预训练-微调的新标准。',
  core_mechanism = '掩码语言模型随机遮蔽15%的token并预测它们，允许模型从双向上下文学习。下一句预测(NSP)帮助理解句子关系。双向Transformer编码器能同时看到整个序列的左右信息。',
  why_it_wins = 'BERT证明了双向预训练的巨大优势，主导了NLP基准测试数年。预训练/微调范式成为标准，激发了大量变体(RoBERTa、ALBERT等)。它让NLP从任务特定模型转向通用预训练模型，为大语言模型时代铺平道路。'
WHERE id = 9;
EOF

# 论文10: GPT-3 (2020)
sqlite3 data/ai_saga.db << 'EOF'
UPDATE papers SET
  prev_paradigm = '2018-2020年，预训练+微调是标准方法。微调需要任务特定数据和计算资源，限制了模型在新任务上的应用。少样本学习虽然被研究，但效果有限。',
  core_contribution = '证明了超大规模(175B参数)语言模型可以实现无需微调的零样本和少样本学习。模型仅从提示中的几个示例就能推断任务并执行，展现了涌现能力。',
  core_mechanism = '简单扩大Transformer规模到前所未有的175B参数，在300B token上训练。上下文学习让模型从提示示例中理解任务，无需梯度更新。模型学习模式识别和任务推断能力。',
  why_it_wins = 'GPT-3引入了上下文学习新范式，点燃了大语言模型时代。它证明了规模化导致涌现能力，为ChatGPT和现代AI助手奠定基础。从此AI从专用系统走向通用助手，开启了AI应用的全新时代。'
WHERE id = 10;
EOF

echo "论文内容补齐完成！"
